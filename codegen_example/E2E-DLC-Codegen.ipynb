{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b044760",
   "metadata": {},
   "source": [
    "# Codegen Sagemaker inference with Intel optimizations\n",
    "\n",
    "## Agenda\n",
    "0. Prerequisites\n",
    "1. Build Deep Learning Container and push it to AWS ECR\n",
    "2. Create a Torchserve file and put it on S3 bucket\n",
    "3. Create AWS Sagemaker endpoint\n",
    "4. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832c793",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Install all libraries required to run the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41ae516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.32.54)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.34.54)\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.34.54)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: s3transfer in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.10.0)\n",
      "Requirement already satisfied: torch-model-archiver==0.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: torchserve==0.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-model-archiver==0.8.1) (0.0.3)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchserve==0.8.2) (10.2.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchserve==0.8.2) (5.9.8)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchserve==0.8.2) (21.3)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchserve==0.8.2) (0.42.0)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (6.0.1)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->torchserve==0.8.2) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.175.0\" --upgrade --quiet\n",
    "! pip install awscli boto3 botocore numpy s3transfer torch-model-archiver==0.8.1 torchserve==0.8.2 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799be805",
   "metadata": {},
   "source": [
    "Remember also that you have all required accesses on you AWS account. To run this example you're going to need following accesses:\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "- AmazonEC2FullAccess\n",
    "- AmazonS3FullAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a6ca7d",
   "metadata": {},
   "source": [
    "### Build Deep Learning Container and push it to AWS ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79e0d4",
   "metadata": {},
   "source": [
    "If you don't have Docker image prepared beforehand, clone the Deep Learning Containers repository and build the image with all required intel optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9eb751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deep-learning-containers' already exists and is not an empty directory.\n",
      "Already on 'intel_pytorch_ipex'\n",
      "Your branch is up to date with 'origin/intel_pytorch_ipex'.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aalbersk/deep-learning-containers\n",
    "!cd deep-learning-containers && git checkout intel_pytorch_ipex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b91d89",
   "metadata": {},
   "source": [
    "By default the image will build `2.2` version of Pytorch+IPEX image. If you'd like to build another version, modify fields `version` and `short_version` in [pytorch/inference/buildspec-intel.yml](https://github.com/aalbersk/deep-learning-containers/blob/intel_pytorch_ipex/pytorch/inference/buildspec-intel.yml). The command below will automatically build the image and push it into your ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6504a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd deep-learning-containers && PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/src INTEL_DEDICATED=true python src/main.py --buildspec pytorch/inference/buildspec-intel.yml --framework pytorch --image_types inference --device_types cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e958d",
   "metadata": {},
   "source": [
    "### Create a Torchserve file and put it on S3 bucket\n",
    "\n",
    "# **<span style=\"background: yellow\">Todo: plan how to get the model and describe it</span>**\n",
    "\n",
    "If you'd like to use your own version of Codegen, here's how to create a torchserve file and put it on S3 bucket.\n",
    "\n",
    "As default Intel DLC has only essential Pytorch libraries + latest Transformers (4.37), Codegen requires requirements with following libraries additionaly:\n",
    "```python\n",
    "transformers==4.33.2\n",
    "tiktoken\n",
    "```\n",
    "\n",
    "To generate a Torchserve MAR file use following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e12d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver --model-name codegen25 --version 1.0 --handler codegen_handler.py --config-file model-config.yaml --extra-files codegen25.py -r requirements.txt --archive-format tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b41a7",
   "metadata": {},
   "source": [
    "Next, copy the model into an S3 bucket of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp codegen25.tar.gz s3://<s3 bucket name>/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95818214",
   "metadata": {},
   "source": [
    "### Create AWS Sagemaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c725f",
   "metadata": {},
   "source": [
    "# **<span style=\"background: yellow\">Todo: describe creating the endpoint</span>**\n",
    "# **<span style=\"background: yellow\">Todo: prepare variables to change based on user needs</span>**\n",
    "# **<span style=\"background: yellow\">Todo: initialize with codegen not bert</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370b11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb019cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account=205130860845, region=us-west-2, role=arn:aws:iam::205130860845:role/sagemaker_fullaccess, output_path=s3://sagemaker-us-west-2-205130860845/torchserve\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto3_session = boto3.session.Session(region_name=\"us-west-2\")\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)\n",
    "region = sess._region_name\n",
    "account = sess.account_id()\n",
    "\n",
    "bucket_name = sess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "print(f'account={account}, region={region}, role={role}, output_path={output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2783d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-ipex-2024-03-04-17-46-00-450\n",
      "<sagemaker.model.Model object at 0x7f3dc5adfdf0>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model\n",
    "\n",
    "instance_type = \"ml.m7i.8xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"bert-ipex\")\n",
    "s3_url = \"s3://intel-sagemaker/bert_ts_clean.tar.gz\"\n",
    "\n",
    "container = \"205130860845.dkr.ecr.us-west-2.amazonaws.com/pytorch_inference:2.2.0-cpu-intel-py310-ubuntu20.04-sagemaker-2024-02-28-13-36-20\"\n",
    "model = Model(\n",
    "    name=\"torchserve-bert-ipex\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    # Enable SageMaker uncompressed model artifacts\n",
    "    model_data=s3_url,\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    env={\"TS_INSTALL_PY_DEP_PER_MODEL\": \"true\",\n",
    "         \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"0\",\n",
    "         \"SAGEMAKER_REGION\": region},\n",
    ")\n",
    "print(endpoint_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872d90f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    #volume_size=32, # increase the size to store large model\n",
    "    model_data_download_timeout=3600, # increase the timeout to download large model\n",
    "    container_startup_health_check_timeout=600, # increase the timeout to load large model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1b6ed",
   "metadata": {},
   "source": [
    "### Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1aea44",
   "metadata": {},
   "source": [
    "# **<span style=\"background: yellow\">Todo: describe invoking the endpoint</span>**\n",
    "# **<span style=\"background: yellow\">Todo: use humaneval to generate</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3401246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.3708372116088867 seconds ---\n",
      "\n",
      " was named the nfl most valuable player ( mvp ). they defeated the arizona cardinals 49 - 15 in the nfc championship game and advanced to their second super bowl appearance since the franchise was founded in 1995. the broncos finis \n",
      "\n",
      " ed the regular season with a 12 - 4 record, and denied the new england patriots a chance to defen \n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "context=\"The Panthers finished the regular season with a 15-1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49-15 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12-4 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20-18 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.\"\n",
    "\n",
    "question=\"Who was named the MVP?\"\n",
    "\n",
    "custom_attributes = \"c000b4f9-df62-4c85-a0bf-7c525f9104a4\"  # An example of a trace ID.\n",
    "# endpoint_name = \"dlc-test\"                               # Your endpoint name.\n",
    "content_type = \"application/json\"                           # The MIME type of the input data in the request body.\n",
    "accept = \"*/*\"                                              # The desired MIME type of the inference in the response.\n",
    "\n",
    "import io\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        data = self.buff.getvalue()\n",
    "        \n",
    "    def scan_lines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0\n",
    "\n",
    "start_time = time.time()\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    CustomAttributes=custom_attributes, \n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=json.dumps({'context': context, 'question': question})\n",
    "    )\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "parser = Parser()\n",
    "for event in response['Body']:\n",
    "    parser.write(event['PayloadPart']['Bytes'])\n",
    "    for line in parser.scan_lines():\n",
    "        print(\"\\n\", line.decode(\"utf-8\"), end=' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e812c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
